{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_equations(allowed_operators, dataset_size, min_value, max_value):\n",
    "    \"\"\"Generates pairs of equations and solutions to them.\n",
    "    \n",
    "       Each equation has a form of two integers with an operator in between.\n",
    "       Each solution is an integer with the result of the operaion.\n",
    "    \n",
    "        allowed_operators: list of strings, allowed operators.\n",
    "        dataset_size: an integer, number of equations to be generated.\n",
    "        min_value: an integer, min value of each operand.\n",
    "        max_value: an integer, max value of each operand.\n",
    "\n",
    "        result: a list of tuples of strings (equation, solution).\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    allowed_operators[1]\n",
    "    for _ in range(dataset_size):\n",
    "        input_= (str(random.randint(min_value, max_value)) + allowed_operators[random.randint(0,len(allowed_operators)-1)] + str(random.randint(min_value, max_value)))\n",
    "        output_= str(eval(input_))\n",
    "        sample.append((input_,output_))\n",
    "    return sample\n",
    "\n",
    "\n",
    "def test_generate_equations():\n",
    "    allowed_operators = ['+', '-']\n",
    "    dataset_size = 10\n",
    "    for (input_, output_) in generate_equations(allowed_operators, dataset_size, 0, 100):\n",
    "        if not (type(input_) is str and type(output_) is str):\n",
    "            return \"Both parts should be strings.\"\n",
    "        if eval(input_) != int(output_):\n",
    "            return \"The (equation: {!r}, solution: {!r}) pair is incorrect.\".format(input_, output_)\n",
    "    return \"Tests passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_generate_equations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_operators = ['+', '-']\n",
    "dataset_size = 100000\n",
    "data = generate_equations(allowed_operators, dataset_size, min_value=0, max_value=9999)\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {symbol:i for i, symbol in enumerate('^$#+-1234567890')}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = '^'\n",
    "end_symbol = '$'\n",
    "padding_symbol = '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(sentence, word2id, padded_len):\n",
    "    \"\"\" Converts a sequence of symbols to a padded sequence of their ids.\n",
    "    \n",
    "      sentence: a string, input/output sequence of symbols.\n",
    "      word2id: a dict, a mapping from original symbols to ids.\n",
    "      padded_len: an integer, a desirable length of the sequence.\n",
    "\n",
    "      result: a tuple of (a list of ids, an actual length of sentence).\n",
    "    \"\"\"\n",
    "    \n",
    "    sent_ids = [word2id[char] for char in sentence]\n",
    "    if(len(sentence)<padded_len):\n",
    "        sent_ids.append(word2id['$'])\n",
    "        for x in range(padded_len-len(sentence)-1):\n",
    "            sent_ids.append(word2id['#'])\n",
    "        return sent_ids,len(sentence)+1    \n",
    "    sent_ids[padded_len-1]=word2id['$']\n",
    "    sent_len = padded_len\n",
    "    \n",
    "    return sent_ids, sent_len\n",
    "\n",
    "\n",
    "def test_sentence_to_ids():\n",
    "    sentences = [(\"123+123\", 7), (\"123+123\", 8), (\"123+123\", 10)]\n",
    "    expected_output = [([5, 6, 7, 3, 5, 6, 1], 7), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 1], 8), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 1, 2, 2], 8)] \n",
    "    for (sentence, padded_len), (sentence_ids, expected_length) in zip(sentences, expected_output):\n",
    "        output, length = sentence_to_ids(sentence, word2id, padded_len)\n",
    "        if output != sentence_ids:\n",
    "            return(\"Convertion of '{}' for padded_len={} to {} is incorrect.\".format(\n",
    "                sentence, padded_len, output))\n",
    "        if length != expected_length:\n",
    "            return(\"Convertion of '{}' for padded_len={} has incorrect actual length {}.\".format(\n",
    "                sentence, padded_len, length))\n",
    "    return(\"Tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence_to_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, id2word):\n",
    "    \"\"\" Converts a sequence of ids to a sequence of symbols.\n",
    "    \n",
    "          ids: a list, indices for the padded sequence.\n",
    "          id2word:  a dict, a mapping from ids to original symbols.\n",
    "\n",
    "          result: a list of symbols.\n",
    "    \"\"\"\n",
    " \n",
    "    return [id2word[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    \"\"\"Prepares batches of indices. \n",
    "    \n",
    "       Sequences are padded to match the longest sequence in the batch,\n",
    "       if it's longer than max_len, then max_len is used instead.\n",
    "\n",
    "        sentences: a list of strings, original sequences.\n",
    "        word2id: a dict, a mapping from original symbols to ids.\n",
    "        max_len: an integer, max len of sequences allowed.\n",
    "\n",
    "        result: a list of lists of ids, a list of actual lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(samples, batch_size=64):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ('4289-8224', '-3935')\n",
      "Ids: [[8, 6, 12, 13, 4, 12, 6, 6, 8, 1], [4, 7, 13, 7, 9, 1, 2, 2, 2, 2]]\n",
      "Sentences lengths: [10, 6]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_set[0]\n",
    "ids, sent_lens = batch_to_ids(sentences, word2id, max_len=10)\n",
    "print('Input:', sentences)\n",
    "print('Ids: {}\\nSentences lengths: {}'.format(ids, sent_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "    \n",
    "    # Placeholders for input and its actual lengths.\n",
    "    self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "    self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "    \n",
    "    # Placeholders for groundtruth and its actual lengths.\n",
    "    self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "    self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "        \n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, vocab_size, embeddings_size):\n",
    "    \"\"\"Specifies embeddings layer and embeds an input batch.\"\"\"\n",
    "     \n",
    "    random_initializer = tf.random_uniform((vocab_size, embeddings_size), -1.0, 1.0)\n",
    "    self.embeddings = tf.Variable(initial_value=random_initializer,name=\"embeddings_random_initialized\",dtype=tf.float32)\n",
    "    \n",
    "    # Perform embeddings lookup for self.input_batch. \n",
    "    self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings,self.input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__create_embeddings = classmethod(create_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(self, hidden_size):\n",
    "    \"\"\"Specifies encoder architecture and computes its output.\"\"\"\n",
    "    \n",
    "    # Create GRUCell with dropout.\n",
    "    encoder_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "                                                  ,input_keep_prob=self.dropout_ph\n",
    "                                                  ,dtype=tf.float32)\n",
    "    \n",
    "    # Create RNN with the predefined cell.\n",
    "    _, self.final_encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell,\n",
    "    self.input_batch_embedded,\n",
    "    sequence_length=self.input_batch_lengths,\n",
    "    dtype=tf.float32\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_encoder = classmethod(build_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(self, hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id):\n",
    "    \"\"\"Specifies decoder architecture and computes the output.\n",
    "    \n",
    "        Uses different helpers:\n",
    "          - for train: feeding ground truth\n",
    "          - for inference: feeding generated output\n",
    "\n",
    "        As a result, self.train_outputs and self.infer_outputs are created. \n",
    "        Each of them contains two fields:\n",
    "          rnn_output (predicted logits)\n",
    "          sample_id (predictions).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use start symbols as the decoder inputs at the first time step.\n",
    "    batch_size = tf.shape(self.input_batch)[0]\n",
    "    start_tokens = tf.fill([batch_size], start_symbol_id)\n",
    "    ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "    \n",
    "    # Use the embedding layer defined before to lookup embedings for ground_truth_as_input. \n",
    "    self.ground_truth_embedded = tf.nn.embedding_lookup(self.embeddings, ground_truth_as_input)\n",
    "     \n",
    "    # Create TrainingHelper for the train stage.\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded, \n",
    "                                                     self.ground_truth_lengths)\n",
    "    \n",
    "    # Create GreedyEmbeddingHelper for the inference stage.\n",
    "    # You should provide the embedding layer, start_tokens and index of the end symbol.\n",
    "    infer_helper =tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_symbol_id)\n",
    "    \n",
    "  \n",
    "    def decode(helper, scope, reuse=None):\n",
    "        \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "            decoder_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_units=hidden_size,reuse=reuse)\n",
    "                                                  ,input_keep_prob=self.dropout_ph\n",
    "                                                  ,dtype=tf.float32)\n",
    "            \n",
    "            # Create a projection wrapper.\n",
    "            decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, vocab_size, reuse=reuse)\n",
    "            \n",
    "            # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "            # The initial state should be equal to the final state of the encoder!\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper, initial_state=self.final_encoder_state)\n",
    "            \n",
    "            # The first returning argument of dynamic_decode contains two fields:\n",
    "            #   rnn_output (predicted logits)\n",
    "            #   sample_id (predictions)\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=max_iter, \n",
    "                                                              output_time_major=False, impute_finished=True)\n",
    "\n",
    "            return outputs\n",
    "        \n",
    "    self.train_outputs = decode(train_helper, 'decode')\n",
    "    self.infer_outputs = decode(infer_helper, 'decode', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_decoder = classmethod(build_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self):\n",
    "    \"\"\"Computes sequence loss (masked cross-entopy loss with logits).\"\"\"\n",
    "    \n",
    "    weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "    \n",
    "    self.loss = tf.contrib.seq2seq.sequence_loss(self.train_outputs.rnn_output,self.ground_truth,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies train_op that optimizes self.loss.\"\"\"\n",
    "    \n",
    "    self.train_op =tf.contrib.layers.optimize_loss(\n",
    "        loss=self.loss,\n",
    "        optimizer='Adam',\n",
    "        learning_rate=self.learning_rate_ph,\n",
    "        clip_gradients=1.0,\n",
    "        global_step=tf.train.get_global_step()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocab_size, embeddings_size, hidden_size, \n",
    "               max_iter, start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "    \n",
    "    self.__declare_placeholders()\n",
    "    self.__create_embeddings(vocab_size, embeddings_size)\n",
    "    self.__build_encoder(hidden_size)\n",
    "    self.__build_decoder(hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id)\n",
    "    \n",
    "    # Compute loss and back-propagate.\n",
    "    self.__compute_loss()\n",
    "    self.__perform_optimization()\n",
    "    \n",
    "    # Get predictions for evaluation.\n",
    "    self.train_predictions = self.train_outputs.sample_id\n",
    "    self.infer_predictions = self.infer_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "    pred, loss, _ = session.run([\n",
    "            self.train_predictions,\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, X, X_seq_len):\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len\n",
    "    }\n",
    "    pred = session.run([\n",
    "            self.infer_predictions\n",
    "        ], feed_dict=feed_dict)[0]\n",
    "    return pred\n",
    "\n",
    "def predict_for_batch_with_loss(self, session, X, X_seq_len, Y, Y_seq_len):\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len\n",
    "    }\n",
    "    pred, loss = session.run([\n",
    "            self.infer_predictions,\n",
    "            self.loss,\n",
    "        ], feed_dict=feed_dict)\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.predict_for_batch = classmethod(predict_for_batch)\n",
    "Seq2SeqModel.predict_for_batch_with_loss = classmethod(predict_for_batch_with_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = model = Seq2SeqModel(\n",
    "    vocab_size = len(word2id),\n",
    "    embeddings_size=20,\n",
    "    max_iter=7,\n",
    "    hidden_size=512,\n",
    "    start_symbol_id=word2id['^'],\n",
    "    end_symbol_id=word2id['$'],\n",
    "    padding_symbol_id = word2id['#']\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "dropout_keep_probability = 0.5\n",
    "max_len = 20\n",
    "\n",
    "n_step = int(len(train_set) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "Train: epoch 1\n",
      "Epoch: [1/10], step: [1/625], loss: 2.734513\n",
      "Epoch: [1/10], step: [201/625], loss: 1.795438\n",
      "Epoch: [1/10], step: [401/625], loss: 1.749122\n",
      "Epoch: [1/10], step: [601/625], loss: 1.678461\n",
      "Test: epoch 1 loss: 1.6372898\n",
      "X: 3764-5447$\n",
      "Y: -1683$\n",
      "O: -189$^\n",
      "\n",
      "X: 3036-1590$\n",
      "Y: 1446$#\n",
      "O: 2899$^\n",
      "\n",
      "X: 9373+8004$\n",
      "Y: 17377$\n",
      "O: 17700$\n",
      "\n",
      "Train: epoch 2\n",
      "Epoch: [2/10], step: [1/625], loss: 1.687203\n",
      "Epoch: [2/10], step: [201/625], loss: 1.565951\n",
      "Epoch: [2/10], step: [401/625], loss: 1.526545\n",
      "Epoch: [2/10], step: [601/625], loss: 1.480577\n",
      "Test: epoch 2 loss: 1.4597286\n",
      "X: 3195+5885$\n",
      "Y: 9080$#\n",
      "O: 9706$^\n",
      "\n",
      "X: 3667+6634$\n",
      "Y: 10301$\n",
      "O: 10166$\n",
      "\n",
      "X: 8477+9961$\n",
      "Y: 18438$\n",
      "O: 17177$\n",
      "\n",
      "Train: epoch 3\n",
      "Epoch: [3/10], step: [1/625], loss: 1.491660\n",
      "Epoch: [3/10], step: [201/625], loss: 1.469961\n",
      "Epoch: [3/10], step: [401/625], loss: 1.468264\n",
      "Epoch: [3/10], step: [601/625], loss: 1.382922\n",
      "Test: epoch 3 loss: 1.348758\n",
      "X: 1452-6156$\n",
      "Y: -4704$\n",
      "O: -4823$\n",
      "\n",
      "X: 4056-2062$\n",
      "Y: 1994$#\n",
      "O: 2233$^\n",
      "\n",
      "X: 3965+9203$\n",
      "Y: 13168$\n",
      "O: 12433$\n",
      "\n",
      "Train: epoch 4\n",
      "Epoch: [4/10], step: [1/625], loss: 1.386866\n",
      "Epoch: [4/10], step: [201/625], loss: 1.372563\n",
      "Epoch: [4/10], step: [401/625], loss: 1.358721\n",
      "Epoch: [4/10], step: [601/625], loss: 1.386042\n",
      "Test: epoch 4 loss: 1.3399802\n",
      "X: 6382-342$#\n",
      "Y: 6040$#\n",
      "O: 5866$^\n",
      "\n",
      "X: 7244-2284$\n",
      "Y: 4960$#\n",
      "O: 4883$^\n",
      "\n",
      "X: 9584-4677$\n",
      "Y: 4907$#\n",
      "O: 5266$^\n",
      "\n",
      "Train: epoch 5\n",
      "Epoch: [5/10], step: [1/625], loss: 1.352690\n",
      "Epoch: [5/10], step: [201/625], loss: 1.333256\n",
      "Epoch: [5/10], step: [401/625], loss: 1.298492\n",
      "Epoch: [5/10], step: [601/625], loss: 1.315452\n",
      "Test: epoch 5 loss: 1.2571827\n",
      "X: 9585+961$#\n",
      "Y: 10546$\n",
      "O: 10242$\n",
      "\n",
      "X: 954+6094$#\n",
      "Y: 7048$#\n",
      "O: 7055$^\n",
      "\n",
      "X: 1550-6989$\n",
      "Y: -5439$\n",
      "O: -5005$\n",
      "\n",
      "Train: epoch 6\n",
      "Epoch: [6/10], step: [1/625], loss: 1.331711\n",
      "Epoch: [6/10], step: [201/625], loss: 1.334838\n",
      "Epoch: [6/10], step: [401/625], loss: 1.283290\n",
      "Epoch: [6/10], step: [601/625], loss: 1.302086\n",
      "Test: epoch 6 loss: 1.2448708\n",
      "X: 9919-9887$\n",
      "Y: 32$###\n",
      "O: -104$^\n",
      "\n",
      "X: 827-2013$#\n",
      "Y: -1186$\n",
      "O: -1546$\n",
      "\n",
      "X: 930+5709$#\n",
      "Y: 6639$#\n",
      "O: 6681$^\n",
      "\n",
      "Train: epoch 7\n",
      "Epoch: [7/10], step: [1/625], loss: 1.290870\n",
      "Epoch: [7/10], step: [201/625], loss: 1.236395\n",
      "Epoch: [7/10], step: [401/625], loss: 1.209694\n",
      "Epoch: [7/10], step: [601/625], loss: 1.176954\n",
      "Test: epoch 7 loss: 1.1489463\n",
      "X: 4110+1995$\n",
      "Y: 6105$#\n",
      "O: 6111$^\n",
      "\n",
      "X: 318+6494$#\n",
      "Y: 6812$#\n",
      "O: 6910$^\n",
      "\n",
      "X: 4711+823$#\n",
      "Y: 5534$#\n",
      "O: 5477$^\n",
      "\n",
      "Train: epoch 8\n",
      "Epoch: [8/10], step: [1/625], loss: 1.182886\n",
      "Epoch: [8/10], step: [201/625], loss: 1.101205\n",
      "Epoch: [8/10], step: [401/625], loss: 1.106045\n",
      "Epoch: [8/10], step: [601/625], loss: 1.039238\n",
      "Test: epoch 8 loss: 1.008367\n",
      "X: 9945-8216$\n",
      "Y: 1729$#\n",
      "O: 1515$^\n",
      "\n",
      "X: 9100+3095$\n",
      "Y: 12195$\n",
      "O: 12193$\n",
      "\n",
      "X: 8455-3941$\n",
      "Y: 4514$#\n",
      "O: 4505$^\n",
      "\n",
      "Train: epoch 9\n",
      "Epoch: [9/10], step: [1/625], loss: 1.023628\n",
      "Epoch: [9/10], step: [201/625], loss: 1.007337\n",
      "Epoch: [9/10], step: [401/625], loss: 1.064651\n",
      "Epoch: [9/10], step: [601/625], loss: 0.991773\n",
      "Test: epoch 9 loss: 0.97741663\n",
      "X: 1809+2151$\n",
      "Y: 3960$#\n",
      "O: 3985$^\n",
      "\n",
      "X: 4734-5973$\n",
      "Y: -1239$\n",
      "O: -1255$\n",
      "\n",
      "X: 4214+6456$\n",
      "Y: 10670$\n",
      "O: 10692$\n",
      "\n",
      "Train: epoch 10\n",
      "Epoch: [10/10], step: [1/625], loss: 0.984659\n",
      "Epoch: [10/10], step: [201/625], loss: 0.999800\n",
      "Epoch: [10/10], step: [401/625], loss: 0.953381\n",
      "Epoch: [10/10], step: [601/625], loss: 0.975934\n",
      "Test: epoch 10 loss: 0.99569285\n",
      "X: 9042-3638$\n",
      "Y: 5404$#\n",
      "O: 5412$^\n",
      "\n",
      "X: 5915+468$#\n",
      "Y: 6383$#\n",
      "O: 6416$^\n",
      "\n",
      "X: 5504-1408$\n",
      "Y: 4096$#\n",
      "O: 4114$^\n",
      "\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "            \n",
    "invalid_number_prediction_counts = []\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):  \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    print('Train: epoch', epoch + 1)\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size=batch_size)):\n",
    "        X_ids, X_sent_lens = batch_to_ids(X_batch, word2id, max_len=max_len)\n",
    "        Y_ids, Y_sent_lens = batch_to_ids(Y_batch, word2id, max_len=max_len)\n",
    "        # prepare the data (X_batch and Y_batch) for training\n",
    "        # using function batch_to_ids\n",
    "        predictions, loss = model.train_on_batch(session,X_ids, X_sent_lens,Y_ids,Y_sent_lens,learning_rate,dropout_keep_probability)\n",
    "        \n",
    "        if n_iter % 200 == 0:\n",
    "            print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" % (epoch + 1, n_epochs, n_iter + 1, n_step, loss))\n",
    "                \n",
    "    X_sent, Y_sent = next(generate_batches(test_set, batch_size=batch_size))\n",
    "    X, X_sent_lens = batch_to_ids(X_sent, word2id, max_len=max_len)\n",
    "    Y, Y_sent_lens = batch_to_ids(Y_sent, word2id, max_len=max_len)\n",
    "    # prepare test data (X_sent and Y_sent) for predicting \n",
    "    # quality and computing value of the loss function\n",
    "    # using function batch_to_ids\n",
    "    \n",
    "    predictions, loss = model.predict_for_batch_with_loss(session,X,X_sent_lens,Y,Y_sent_lens)\n",
    "    print('Test: epoch', epoch + 1, 'loss:', loss,)\n",
    "    for x, y, p  in list(zip(X, Y, predictions))[:3]:\n",
    "        print('X:',''.join(ids_to_sentence(x, id2word)))\n",
    "        print('Y:',''.join(ids_to_sentence(y, id2word)))\n",
    "        print('O:',''.join(ids_to_sentence(p, id2word)))\n",
    "        print('')\n",
    "\n",
    "    model_predictions = []\n",
    "    ground_truth = []\n",
    "    invalid_number_prediction_count = 0\n",
    "    # For the whole test set calculate ground-truth values (as integer numbers)\n",
    "    # and prediction values (also as integers) to calculate metrics.\n",
    "    # If generated by model number is not correct (e.g. '1-1'), \n",
    "    # increase invalid_number_prediction_count and don't append this and corresponding\n",
    "    # ground-truth value to the arrays.\n",
    "    for X_batch, Y_batch in generate_batches(test_set, batch_size=batch_size):\n",
    "        y_sent = ''.join(ids_to_sentence(y, id2word))\n",
    "        y_sent = y_sent[:y_sent.find('$')]\n",
    "        p_sent = ''.join(ids_to_sentence(p, id2word))\n",
    "        p_sent = p_sent[:p_sent.find('$')]\n",
    "        if p_sent.isdigit() or (p_sent.startswith('-') and p_sent[1:].isdigit()):\n",
    "            model_predictions.append(int(p_sent))\n",
    "            ground_truth.append(int(y_sent))\n",
    "        else:\n",
    "            invalid_number_prediction_count += 1\n",
    "    \n",
    "    all_model_predictions.append(model_predictions)\n",
    "    all_ground_truth.append(ground_truth)\n",
    "    invalid_number_prediction_counts.append(invalid_number_prediction_count)\n",
    "            \n",
    "print('\\n...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, MAE: 323.000000, Invalid numbers: 0\n",
      "Epoch: 2, MAE: 1261.000000, Invalid numbers: 0\n",
      "Epoch: 3, MAE: 735.000000, Invalid numbers: 0\n",
      "Epoch: 4, MAE: 359.000000, Invalid numbers: 0\n",
      "Epoch: 5, MAE: 434.000000, Invalid numbers: 0\n",
      "Epoch: 6, MAE: 42.000000, Invalid numbers: 0\n",
      "Epoch: 7, MAE: 57.000000, Invalid numbers: 0\n",
      "Epoch: 8, MAE: 9.000000, Invalid numbers: 0\n",
      "Epoch: 9, MAE: 22.000000, Invalid numbers: 0\n",
      "Epoch: 10, MAE: 18.000000, Invalid numbers: 0\n"
     ]
    }
   ],
   "source": [
    "for i, (gts, predictions, invalid_number_prediction_count) in enumerate(zip(all_ground_truth,\n",
    "                                                                            all_model_predictions,\n",
    "                                                                            invalid_number_prediction_counts), 1):\n",
    "    mae = mean_absolute_error(gts, predictions)\n",
    "    print(\"Epoch: %i, MAE: %f, Invalid numbers: %i\" % (i, mae, invalid_number_prediction_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
